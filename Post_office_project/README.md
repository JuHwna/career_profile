# 우체국예금 차세대 프로젝트
## 1. 분석 실행 과제 목록

|구분|분석주제|과제 번호|실행 과제명|설명|
|---|------|-------|--------|---|
|우선 실행과제|분석 기반 상품 추천|1|상품군 보유기반 분석기반 금융상품 추천||
|||2|접촉정보 분석기반 고객 관심상품 추천||
|||3|구매패턴 분석기반 금융상품 교차 추천||
|||4|신규고객 분석기반 상품추천||
|||5|카드사용패턴 분석기반 체크카드 상품 추천||
|1차 추진 실행과제|고객 분석|6|예금/보험/우편 통합고객에 대한 상품/서비스 다차원 결합 분석||
|||7|고객정보/접촉정보/거래정보 기반 고객 세분화 분석||
|||8|이탈고객 세그먼트 도출 및 잠재 이탈 고객 예측||
|||9|우수고객에 대한 수익성 분석 모델 개발||


## 2. 기간
- 2022.01.01 ~ 2022.09.30


## 3. 맡은 일 
- 1번, 3번 과제 추천 모델 개발 및 태블로 화면 개발
- 고객 클러스터링 진행 및 고객 세분화 

## 4. 모델 개발 과정

### (1) 클러스터링 
- 6번, 7번 과제를 진행해야 모델을 개발할 수가 있어서 먼저 고객 분석을 시도
- 고객 분석 후, 군집을 추출하여 특성을 부여해야 하기 때문에 클러스터링을 진행
- 클러스터링 진행을 위해 데이터 전처리
  - 각 상품별로 중요하다고 판단되는 변수를 다 넣기로 했음
    - 공통 변수 :  SEX_CD(성별), AGE_DVSN_CD(연령대), OCPT_SPLIT(경제활동여부), grp_D(예금고객여부), grp_I(보험고객여부), grp_C(카드고객여부), grp_F(펀드고객여부)
    - 예금 : HD(거치식?), DD(적립식?), SD(요구불?), AA(전체계좌 개수), DPST_NOCS(입금 횟수), DPST_AMT(입금 금액), GIVE_NOCS(출금 횟수), GIVE_AMT(출금 금액), PSRV_RAMT, M3_ABAL1(3개월 평잔), YEAR_ABAL1(연평잔), OVRD_CNT, ARRR_NOCS, ATPM_STNG_NOCS, AUTR_STNG_NOCS
    - 보험 : 사망(사망보험보유개수), 상해(상해보험보유개수), 질병(질병보험보유개수), 어린이(어린이보험보유개수), 단체(단체보험보유개수), 교육(교육보험보유개수), 비적격(비적격보험보유개수), 적격(적격보험보유개수), 저축성(저축성보험보유개수), 특약(특약보험보유개수), 보험전체(전체보험보유개수)
    - 펀드 : 여기서 펀드는 고객 수가 매우 작아서 클러스터링하면 특징이 안나올 거 같아 하지 않았음
- 전처리 중 새로운 변수 만든 것들
  - 직업 코드의 경우, 직업이 너무 많아 이대로 변수로 사용하기 애매했음
    - 그래서 직업 코드가 아닌 경제인, 비경제인, 알수 없음으로 나누어 데이터 생성
  - 고객마다 보유한 금융상품에 대해서 구분하고자 grp_TYPE으로 나누어 데이터 생성
  - 연령의 경우, 연령대로 바꾸어서 데이터 생성(90세 이상인 고객들은 하나로 통일)

- 이후 전처리에서 발견된 것들
  - 상품을 보유한 개수와 보유 금액이 극과 극인 경우가 많았음

- 클러스터링을 위해 사용한 모델들 : k-means, k-prototype
  - 문제점들
    - k-means의 경우, 이산형과 catogorical을 반영하기에 적절하지 않음, 많은 데이터임에도 속도가 그렇게 느리지 않음
    - k-prototype의 경우, 이산형과 catogorical, numeric 섞여 있는 데이터에서도 사용이 가능하여 제일 적합함. 하지만 데이터가 커질수록 속도가 느려지는 단점이 발생(우체국 데이터에서도 일부 샘플로만 돌려봤을 때 엄청 오래 걸리는 것을 확인)
```python
from kmodes.kprototypes import KPrototypes
kproto=KPrototypes(n_clusters=3,init='Huang',verbose=2,max_iter=20,random_state=0,n_init=5)
test=kproto.fit(df2,categorical=[ 0,   2,  3,  4,  5,  6,  7, 8,9])
```
  - 해결방안
    - k-prototype보다 상대적으로 매우 빠른 k-means로 선택 -> 이산형과 catogorical 변수는 원핫 인코딩이나 숫자로 바꿔서 k-means 진행

- K-means를 사용하여 클러스터링 진행
  - 앞서 말한 대로 범주형과 카테고리형 데이터는 숫자로 변형하여 진행
  - 수치형 데이터는 minmax를 통해 정규화? 진행
```python
scaler=MinMaxScaler()
ss=scaler.fit_transform(df_de.loc[:,['HD','DD','SD','AA','DPST_NOCS','GIVE_NOCS','OVRD_CNT','DPST_AMT','GIVE_AMT',
                                     'PSRV_RAMT', 'M3_ABAL1',
                                    'ARRR_NOCS','ATPM_STNG_NOCS','AUTR_STNG_NOCS']])
df_de_num=pd.DataFrame(ss,columns=['HD','DD','SD','AA','DPST_NOCS','GIVE_NOCS','OVRD_CNT','DPST_AMT','GIVE_AMT',
                                     'PSRV_RAMT', 'M3_ABAL1','ARRR_NOCS','ATPM_STNG_NOCS','AUTR_STNG_NOCS'])
```
  - 클러스터링 진행 시(random_state=0으로 고정)
    - 처음에는 모든 변수 넣고 임의로 k 개수 지정하여 k-means 돌린 후 적절하게 군집 나누어졌는지 확인
```python
model=KMeans(n_clusters=8,random_state=0)
model.fit(de)
predict=pd.DataFrame(model.predict(de))
predict.columns=['predict']
```


  - 결과는 적당히 나오긴 했지만 애매하게 나왔음
- 이후 대표님 요청에 의해 고객 정보 변수들만, 금융거래정보만, 로열티 정보만을 통해 k-means 돌려봄
  - 결과는 좋지 않게 나옴
- 다시 처음에 했던 방식으로 돌아와서 k 개수를 리스트로 지정하여 k-means를 돌려 제일 많이 꺾이는 부분 결과 보기
  - 해당 방법을 통해 설명하기 괜찮은 군집으로 나누어졌다고 판단
    - 예금은 8개의 군집, 보험은 7개의 군집
        
 - 하지만 해당 clustering은 사용되지 못했음
   - 대표님이 컷함
     - 이유
       - 너무 나이와 성별로만 나누어짐, 3개월 평잔 등은 모두 다양하게 분포되어 있음
       - 마이크로 세그를 할 때 나이와 성별을 쓰는데 벌써부터 나누어지면 마이크로 세그 사용하기 애매함
       - 예금에서는 3개월 평잔이 가장 중요한데 나누어지는데 사용되지 않은 거 같음
   - 만든 clustering 결과는 고객 세분화가 아닌 상품 추천 쪽에 사용하기로 
   - 그래서 고객 세분화를 위해 임의로 줄을 그어 군집을 형성(금융상품별로 해당 기준 다름)
     - 예금의 경우, 3개월 평잔과 보유 개수를 사용
       - 3개월 평잔
![image](https://user-images.githubusercontent.com/49123169/195256018-be20566c-1340-4a92-9f6f-a2f3d4e7aeb3.png)
       - 보유 개수
![image](https://user-images.githubusercontent.com/49123169/195256132-fa24578f-6c5e-42da-8cf6-e13eaafe065a.png)


~~~python
def m3_split2(x):
    if x==0:
        return '3개월평잔0원'
    elif x>0 and x<100000:
        return '3개월평잔1원~10만원'
    elif x>=100000 and x<1600000:
        return '3개월평잔10만원~160만원'
    elif x>=1600000 and x<12000000:
        return '3개월평잔160만원~1200만원'
    elif x>=12000000 and x<120000000:
        return '3개월평잔1200만원~1억2천만원'
    elif x>=120000000:
        return '3개월평잔1억2천만원이상'
def aa_split2(x):
    if x==1:
        return '개수1개'
    elif x==2:
        return '개수2개'
    elif x==3:
        return '개수3개'
    elif x>=4:
        return '개수4개이상'
~~~

     - 보험의 경우, 가입한 보험의 종류 여부를 사용

### (2) 상품 추천 모델
#### 연관성 분석
- 제일 쉬운 장바구니 분석 사용해서 보았음
- 오래 걸렸으나 결과는 나왔는데 lift값이 1인 애들이 생각보다 많지 않았음
- 특히 교차에서는 lift 값이 1이 넘지 않은 상품을 추천 받은 고객들이 대다수였음
- 이러한 문제로 연관성 분석은 안 쓰기로 결정함

![image](https://user-images.githubusercontent.com/49123169/195510114-91b61637-2f71-4535-85b3-01c50d9d7e14.png)

![image](https://user-images.githubusercontent.com/49123169/195510182-6f4d50d2-9a52-471c-89f5-746af5da5c46.png)

#### item-based 코사인 유사도
- 대표님의 요청으로 돌려봄
- 펀드 상향 추천을 돌려보았을 때 문제 없이 잘 돌아갔음
  - 성능 평가에서도 왔다갔다 하긴 했는데 잘 나올 때도 있긴 했음
- 예금, 보험에도 적용해보려고 했으나 메모리 부족 에러가 발생
  - 확인해본 결과, 에러에 제시된 용량만큼 램이 있어야 된다고 나와 있음
  - 에러에서는 100기가가 넘는 램이 있어야 된다고 표시됨
  - local이든 서버든 pod든 100기가 넘는 램을 가진 곳이 없어 해당 모델 파기

### ALS 모델
- 새로운 모델 찾다가 ALS 모델 발견
  - sparse한 데이터에 적합하면서 행렬분해를 통해 상품별로 선호도라는 것을 정의해주었음
    - 즉, 0이라는 값이 이 상품을 싫어하는 게 아닌 모르는 경우가 존재하여 선호도를 추가해주었음 
  - 처음에는 tensorflow, pytorch도 아닌 als 모델을 쌩 코드로 짜서 모델을 돌려보았음
    > https://yeomko.tistory.com/8?category=805638
    - 일부 데이터로 모델 돌렸을 때 메모리 문제 없이 잘 돌아가서 모든 데이터를 넣고 돌려봄
    - 코사인 유사처럼 메모리 문제 발생
    - 해당 문제로 다른 모델 찾으려고 했음

~~~python
#쌩코드로 als 돌린 코드
r_lambda=40
nf=200
alpha=40
import numpy as np
R=np.array([데이터])
nu=R.shape[0]
ni=R.shape[1]
X=np.random.rand(nu,nf)*0.01
Y=np.random.rand(ni,nf)*0.01
P=np.copy(R)
P[P>0]=1
C=1+alpha*R

def loss_function(C,P,xTy,X,Y,r_lambda):
  
~~~

  - 그러다가 implicit이라는 패키지를 발견함
    - 해당 패키지는 als모델과 bpr모델을 학습하는데 지원해주는 패키지로 cpu와 gpu 모두 학습이 가능함
      - cpu의 경우, 병렬처리로 학습해서 학습 속도가 더 빠르다고 함
    - 이 패키지에 희망을 걸고 ALS 모델을 연구하기 시작함

#### 1차 고비 : 고객x상품 행렬 메모리 문제
   - CF에서 행렬분해를 사용하기 위해선 고객X상품 행렬로 만들어야 함
   - 상향 추천의 경우, 각 금융상품별로 각 고객을 행으로 금융상품명을 열로 하는 행렬 만드는 시도 진행
     - 별 문제 없이 진행되는 것을 확인
   - 교차 추천에서는 모든 고객을 행으로 모든 금융상품명을 열로 하는 행렬 만드는 시도 진행
     - 메모리가 터지는 문제 발생
     - 일단 펀드의 경우, 고객 수가 매우 적어 다른 사람들에게 교차 추천할만한 거리가 없다고 판단 -> 그래서 펀드 교차 추천만 다른 방식으로 하기로 결정
       - 그래서 펀드 고객과 펀드의 상품명을 제거하는 방안을 진행
     - 교차 추천 해결 방법으로 총 세 가지 방안을 진행
   
![image](https://user-images.githubusercontent.com/49123169/202400742-b79e7755-b748-492d-a059-5cd4311ddbf0.png)

   
~~~python
ss=pd.concat([df_final[df_final['predict']==i]['csno'],pd.get_dummies(df_final[df_final['predict']==i]['b.gds_korn_nm'])],axis=1)
ms=ss.groupby('csno').any()
ms=ms.groupby('csno').sum()
~~~

- 1차 방법
  - 상향 추천에서 고려하려고 했던 클러스터링별로 상향 추천하려는 방식을 교차 추천에도 적용해보려고 시도했음.
  - 문제 발생 : 특정 클러스터링에서 교차 상품을 가진 고객이 없는 것을 발견
    - ex) 예금 고객을 가지고 클러스터링을 진행한 뒤에 해당 클러스터링별로 교차 추천 돌리려고 했을 때 한 군집이 예금 상품만 가지고 있는 것을 발견
  - 이 문제는 이미 클러스터링을 통해 군집화된 고객들을 사람 맘대로 바꿀 수 없기 때문에 해당 방법은 사용하지 않기로 결정
  
- 2차 방법
  - 교차 상품을 가진 고객들을 교차 상품 추천 학습에 사용, 학습한 모델에 교차 상품을 가지지 않은 고객을 넣어서 추천을 뽑아내려고 시도했음
  - 문제 발생 : 교차 상품을 가지지 않은 고객 수가 더 많기 때문에 그 수만큼 학습한 모델에 넣고 결과를 뱉었는데 그 수만큼 나오지 않았음
    - 확인해본 결과, 교차 상품 추천에 사용한 고객 수만큼만 결과를 뽑을 수 있었음
    - 결론 : 상품 추천 학습에 사용한 고객들에게 상품 추천이 가능함. 즉, 상품 추천 학습에 사용하지 않은 고객들은 상품 추천을 받을 수 없음.
  - 해당 문제로 이 방법을 사용하지 않기로 결정
  
- 3차 방법
  - 교차 추천 대상 고객과 교차 상품을 가진 고객들을 교차 상품 추천 학습에 사용. 이후, 교차 상품 추천 결과를 도출할 때 대상 고객들에게만 추천을 진행하려는 시도
  - 교차 추천 대상 고객들의 리스트만 따로 저장만 해놓으면 큰 문제가 없었음
  - 그런데 하나의 문제가 발생함
    - 예금과 보험의 고객 수가 매우 많아 고객x상품 매트릭스 형태로 만들 때 메모리가 터지는 문제 발생
    - 어떨 때는 버텼지만 그 이후 매트릭스에서 교차 추천 학습에 쓰이는 고객들만 추출할 때 메모리가 터졌음
  - 해결 방법
    - 데이터 처리 순서를 바꿈
      - 원래는 고객x상품 매트릭스를 만든 이후, 학습 대상 고객들만 따로 뽑아 학습 매트릭스를 구성했음
      - 이후에는 먼저 학습 대상 고객들을 뽑고 그 이후에 고객x상품 매트릭스를 만들었음
  - 해당 해결 방법을 통해 메모리 문제 없이 교차 추천 가능한 것을 확인했음
    - 그래서 해당 방법으로 교차 추천하기로 결정함

~~~python
df_insu=df_insu[df_insu['csno'].isin(ts)]
df_insu.reset_index(drop=True,inplace=True)
df_depo=df_depo[df_depo['csno'].isin(ts)]
df_depo.reset_index(drop=True,inplace=True)

in_ss=pd.pivot_table(df_insu2,index='csno',columns='c.insu_gds_nm',values='test_num').fillna(0).astype('int8')
de_ss=pd.pivot_table(df_depo2,index='csno',columns='c.gds_korn_nm',values='test_num').fillna(0).astype('int8')    

ms_1=pd.merge(de_ss,in_ss,how='left',on='csno')
de_col=len(de_ss.columns)-1
~~~

#### 2차 고비 : 추천 고객 결과 추출 속도 문제
- 학습 추천 이후, 추천 대상 고객들에게 5개씩 추천 결과를 도출해야 했음
- 해당 과정을 위해 implicit에서 지원하는 함수를 사용하여 각 고객마다 5개씩 뽑아내는 코드 작성

~~~python
for k in range(len(df_nos_csno)):
    rec = als.recommend(k,sparse_user_item2[k],N=5,items=ss_s)[0]
    re=[df_nos.columns.tolist()[gd_id] for gd_id in rec]
    gds_rechange=[depo_gds[depo_gds['c.gds_korn_nm']==j]['c.gds_cd'].iloc[0] for j in re]
    row_list.append(gds_rechange)
~~~

- 위의 코드에서 큰 문제가 발생했음
  - 예금과 보험의 경우, 고객 수가 많아서 for문을 다 도는데 많은 시간이 걸리는 것을 파악
  - 이 때 당시, gpu를 지원하지 않았기 때문에 해당 문제를 해결하지 않으면 배치에서 추천으로만 1일 이상 걸릴 수 있는 문제 파악
  - 해당 문제를 해결하기 위해 여러 방법을 찾아 고안해보았음

- 1차 방법
  - multiprocessing을 사용하여 for문을 여러 코어로 나누어서 사용하려고 했음
  - 코드를 구현하여 진행해본 결과, 4시간 걸리는게 1시간 30분으로 줄어드는 것을 확인했음
    - 그런데 잘 돌아갈 때도 있고 잘 안 돌아갈 때도 있는 문제가 발생
    - 즉, for문을 여러 코어로 나누어서 잘 돌아갈 때는 시간이 줄어들지만 여러 코어로 나누어서 안 돌아갈 때는 기존과 똑같은 시간이 걸리거나 아예 결과를 뱉지 못했음
  - 큰 문제 발생으로 다른 방법을 찾기로 결정

~~~python
def als_rec(x):
    rec = als.recommend(x,sparse_user_item2[x],N=50,items=ss_s)[0]
    re=[ms_1_no.columns.tolist()[gd_id] for gd_id in rec]
    # ALS 모델 학습 결과 추천 상품 코드 5개 상품 한글명으로 변환
    gds_rechange=[depo_gds[depo_gds['c.gds_korn_nm']==j]['c.gds_cd'].iat[0] for j in re]
    gds_rechange2=[i for i in gds_rechange if i in df_depo_real2]
    return gds_rechange2[:5]

a_pool=multiprocessing.Pool(processes=3)
result=a_pool.map(als_rec,range(len(ms_1_no_csno)))
row_list.extend(result)
a_pool.close()
~~~

- 2차 방법
  - numba와 같은 c언어나 c++ 기반으로 돌릴 수 있는 패키지를 사용하여 속도 향상 시도
  - 그런데 해당 방법은 numpy나 각 패키지마다 원하는 데이터 형태로 변환하는 과정이 필요
    - 고객x상품 매트릭스를 해당 형태로 바꾸기에는 한계가 존재했음(데이터프레임 형태가 사용하기 편했음)
    - 데이터프레임 형태를 버린다고 하더라고 오류가 발생하는 것을 확인
      - implicit에서 지원하는 함수가 먹히지 않았음
  - 다른 방법 찾기 시작
- 3차 방법
  - implicit 안에서 numpy 형태로 집어넣으면 결과를 numpy형태로 뱉어주는 함수 추가된 것을 확인
  - 서버에 설치했을 때는 구 버전으로 설치하여 numpy로 추천 대상 고객들을 집어 넣을 수 없었고 한 명의 고객들을 넣어서 뱉어주는 형태로만 추천 결과를 도출할 수 있었음
  - 하지만 1월 25일 정도의 버전부터는 numpy 형태로 추천 대상 고객들을 집어 넣을 수 있어 해당 방법을 사용해봄
    - multiprocessing보다 훨씬 더 빠른 속도를 볼 수 있었음
  - 코드 수정도 쉽고 속도도 매우 빨라 해당 방식 사용

~~~python
rec = als.recommend(select_csno,sparse_user_item[select_csno],N=5,items=re_item)[0]
~~~

### 모델 개발 과정 및 결과
1. 하이브에서 필요한 데이터 추출 작업 진행
- 해당 과정에서 필요한 고객 테이블과 상품 정보 테이블 등을 사용했음 
- 1차 문제점
  - 고객 테이블과 상품 정보 테이블에 담겨진 데이터의 수가 많았고 추천 대상 고객만 걸려도 천만명이라서 파이썬 내로 데이터 프레임 형태로 부르는데 오래 걸리는 이슈가 있었음
    - 부르는데 사용한 패키지는 pyhive였는데 pyhive 패키지 문제인줄 알고 ph2도 사용하여 부르는 속도 측정
    - 두 패키지 부르는데 큰 속도 차이가 없어서 pyhive로 계속 사용
      - 원인이 자원 자체가 크지 않고 서버 전송 속도도 좋지 않아 데이터를 파이썬으로 부르는데 오래 걸리는 거 같았음
      - dbeaver 내에서는 빠른 속도로 검색, 접근 가능했음
      - 아니면 파이썬으로 부르는 거 자체가 느린 걸 수도 있다고 판단함
- 2차 문제점
  - 이쪽 테이블 저쪽 테이블 join하여 부르다보니 하이브 자원을 많이 사용하는 문제가 있었음
    - 이러다보니 추천을 위해 파이썬으로 데이터를 끌어다 쓰는 과정에서 다른 배치를 못 돌릴 수 있는 상황이 발생함
      - 다른 코드들도 동시에 돌려야 하기 때문에 문제가 발생
    - work 테이블을 하나 만들어서 그쪽에 join 테이블을 미리 만들어 놓고 사용하는 방식으로 결정됨
    - 그래서 추천 모델을 만들기 위해 총 8개의 work 테이블이 만들어짐
    
    |work 테이블|||
    
- 해당 문제점을 해결하며 상향추천(예금,보험,펀드)와 교차추천(예금,보험,펀드)에 필요한 고객, 클러스터링, 상품 데이터 부를 수 있었음
- 혹여나 데이터를 부르지 못하거나 불렀음에도 데이터가 0이면 오류코드를 내뱉고 추천을 종료하는 코드를 

- **하나의 요청 사항**
  - 예금과 보험의 경우, 판매가 종료된 상품을 가진 고객들이 대다수가 존재했음
    - 이런 상황에서 추천 결과를 도출할 때 판매가 종료된 상품을 추천하는 문제가 발생했음
  - 예금과 보험 판매종료일자가 담긴 테이블을 찾아 해당 정보를 통해 추천에서 빼기로 결정

2. 데이터 전처리 진행
- 상향 추천
  - 전체 고객 중 각 상품별 상향추천(예금상향추천, 보험상향추천, 펀드상향추천)에 해당되는 고객을 먼저 분류
  - 클러스터링별로 상향 추천을 진행하므로 클러스터링에 필요한 열을 추출하고 수치형 데이터를 minmax로 범주형 데이터를 0,1로 바꾸거나 새로운 데이터 형태로 바꾸는 과정을 거쳤음
  - 그리고 미리 클러스터링을 진행하여 나온 가중치들을 담은 pickle 파일을 사용하여 클러스터링 진행했음
  - 이 과정에서 나온 군집 개수와 기존에 나눈 군집 개수가 맞는지를 확인하여 맞지 않으면 코드를 종료시키는 코드 구현 및 적용
  - 클러스터링별로 추천 학습이 진행되게 클러스터링이 담긴 리스트를 데이터프레임에 넣는 과정을 진행했음
  - 이후의 전처리 과정들은 클러스터링별로 진행했음
    - 이후에는 금융상품을 구매한 시점별로 다른 가중치를 부여(최근에 구매한 상품일수록 더 큰 가중치 부여)
      - 동일한 금융상품을 2번 구입하여 보유하고 있는 고객의 경우, 최근에 구매한 상품의 가중치를 부여하고 이전에 구매한 상품은 학습에 사용하지 않음
    - 고객x상품 매트릭스로 구성하되, 부여한 가중치를 해당 매트릭스에 넣음
      - 고객x상품 매트릭스에서 고객의 경우, 고객번호로 나열한 뒤 고객인덱스로 저장되고 상품쪽은 상품명으로 되어 있는데 각 금융상품별로 하나의 종류 금융상품만 들어가 있음
        - ex) 예금상향추천 - 예금상품명만, 보험상향추천 - 보험상품명만
    - 만들어진 고객x상품 매트릭스를 csr-matrix 형태로 바꾸는 과정 진행
- 교차 추천
  - 전체 고객 중 각 상품별 교차추천(예금교차추천, 보험교차추천, 펀드교차추천)에 해당되는 고객과 학습에 사용되는 고객들을 먼저 분류
  - 교차 추천 학습 고객과 추천 해당 고객을 나누고 교차 추천 해당 고객의 경우에는 고객 번호와 고객 인덱스를 저장
  - 이후, 금융상품을 구매한 시점별로 다른 가중치를 부여(최근에 구매한 상품일수록 더 큰 가중치 부여)
    - 동일한 금융상품을 2번 구입하여 보유하고 있는 고객의 경우, 최근에 구매한 상품의 가중치를 부여하고 이전에 구매한 상품은 학습에 사용하지 않음
  - 고객x상품 매트릭스로 구성하되, 부여한 가중치를 해당 매트릭스에 넣음
    - 고객x상품 매트릭스에서 고객의 경우, 고객번호로 나열한 뒤 고객인덱스로 저장되고 상품쪽은 상품명으로 되어 있는데 예금, 보험상품명이 들어가 있음
      - ex) 예금교차추천 - 예금상품명과 보험상품명 둘다 존재
  - 만들어진 고객x상품 매트릭스를 csr-matrix 형식으로 바꾸는 과정 진행

3. 추천 모델 학습
- 상향 추천
  - 클러스터링별로 추천 모델 학습
    - implicit 패키지에 있는 implicit.als.AlternatingLeastSquares 사용
      - 하이퍼파라미터 입력 및 alpha값을 고객x상품 csr-matrix에 곱하고 데이터 형식을 double로 교체
        - 하이퍼파라미터는 그리드 서치와 같은 방식을 통해 최적의 파라미터 값을 찾아서 넣음
    - fit을 통해 모델 학습
- 교차 추천
  - implicit 패키지에 있는 implicit.als.AlternatingLeastSquares 사용
    - 하이퍼파라미터 입력 및 alpha값을 고객x상품 csr-matrix에 곱하고 데이터 형식을 double로 교체
      - 하이퍼파라미터는 그리드 서치와 같은 방식을 통해 최적의 파라미터 값을 찾아서 넣음

4. 추천 모델 최적의 파라미터 찾기
- implicit.als.AlternatingLeastSquares 내에 존재하는 파라미터들 중에 factors, regularization, iterations, alpha 값을 기준으로 최적의 파라미터를 찾기 시작함
  - 최적의 하이퍼파라미터 찾는 방법
    - 기준 시점을 정해두고 기준 시점에서 두 달 전에 고객이 보유한 상품 정보 데이터를 가지고 각 파라미터 값별로 학습 후, 추천 결과 추출
    - 기준 시점에서 한 달 전의 데이터를 test 데이터로 사용
      - ex) 기준 시점이 12월 3일이면 11월 1일 ~ 11월 30일까지 고객이 구매한 상품 정보들을 test 데이터로 사용
    - 각 파라미터 값별로 학습하여 나온 추천 결과를 test 데이터와 비교하여 정확도 측정
    - 정확도가 높은 값을 최적의 하이퍼 파라미터 값이라고 판단하여 해당 값을 사용

5. 추천 모델 결과 추출
- 상향 추천
  
   
